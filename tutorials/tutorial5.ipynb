{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67dfc6",
   "metadata": {},
   "source": [
    "# Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cliff Walking environment parameters\n",
    "GRID_HEIGHT = 4\n",
    "GRID_WIDTH = 4\n",
    "START_STATE = (0, 0)\n",
    "GOAL_STATE = (3, 3)\n",
    "CLIFF_ONE = (1, 1)\n",
    "CLIFF_TWO = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions: 0: Up, 1: Down, 2: Left, 3: Right\n",
    "ACTIONS = [( -1, 0), (1, 0), (0, -1), (0, 1)] # delta_row, delta_col\n",
    "NUM_ACTIONS = len(ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "REWARD_NORMAL = -1\n",
    "REWARD_GOAL = 10\n",
    "REWARD_CLIFF = -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe71eb6a",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state):\n",
    "    row, col = state\n",
    "    return 0 <= row < GRID_HEIGHT and 0 <= col < GRID_WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state_and_reward(state, action_idx):\n",
    "    row, col = state\n",
    "    delta_row, delta_col = ACTIONS[action_idx]\n",
    "    next_row, next_col = row + delta_row, col + delta_col\n",
    "\n",
    "    if not is_valid_state((next_row, next_col)):\n",
    "        # Stay in the same state if moving out of bounds\n",
    "        next_state = state\n",
    "        reward = REWARD_NORMAL\n",
    "    elif (next_row, next_col) == CLIFF_ONE or (next_row, next_col) == CLIFF_TWO:\n",
    "        # Fell off the cliff\n",
    "        next_state = START_STATE\n",
    "        reward = REWARD_CLIFF + REWARD_NORMAL  # -1 for the step, plus cliff penalty\n",
    "    elif (next_row, next_col) == GOAL_STATE:\n",
    "        # Reached the goal\n",
    "        next_state = GOAL_STATE # Stays at goal after reaching\n",
    "        reward = REWARD_NORMAL # Still -1 per step, but episode ends\n",
    "    else:\n",
    "        # Normal movement\n",
    "        next_state = (next_row, next_col)\n",
    "        reward = REWARD_NORMAL\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, gamma=0.9, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Performs iterative policy evaluation to estimate the state-value function V.\n",
    "    Args:\n",
    "        policy (dict): A dictionary mapping state to action probabilities (or a single action if deterministic).\n",
    "        gamma (float): Discount factor.\n",
    "        theta (float): Threshold for convergence.\n",
    "    Returns:\n",
    "        numpy.ndarray: The estimated state-value function V.\n",
    "    \"\"\"\n",
    "    V = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "\n",
    "    # define the goal state reward\n",
    "    V[GOAL_STATE] = REWARD_GOAL\n",
    "\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for r in range(GRID_HEIGHT):\n",
    "            for c in range(GRID_WIDTH):\n",
    "                state = (r, c)\n",
    "                if state == GOAL_STATE:\n",
    "                    continue # Skip goal state as its value is already set\n",
    "\n",
    "                v = V[r, c]\n",
    "                new_v = 0\n",
    "\n",
    "                # If policy is deterministic (maps state to a single action index)\n",
    "                if isinstance(policy[state], int):\n",
    "                    action_idx = policy[state]\n",
    "                    next_state, reward = get_next_state_and_reward(state, action_idx)\n",
    "                    new_v += reward + gamma * V[next_state]\n",
    "                # If policy is stochastic (maps state to a probability distribution over actions)\n",
    "                else:\n",
    "                    for action_idx in range(NUM_ACTIONS):\n",
    "                        prob = policy[state].get(action_idx, 0) # Get probability for action\n",
    "                        next_state, reward = get_next_state_and_reward(state, action_idx)\n",
    "                        new_v += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "                V[r, c] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Define a simple policy (e.g., always move Right unless at the edge)\n",
    "# This is a deterministic policy for demonstration\n",
    "deterministic_policy = {}\n",
    "for r in range(GRID_HEIGHT):\n",
    "    for c in range(GRID_WIDTH):\n",
    "        state = (r, c)\n",
    "        if state == GOAL_STATE:\n",
    "            deterministic_policy[state] = 0 # Action doesn't matter at goal\n",
    "        elif c < GRID_WIDTH - 1:\n",
    "            deterministic_policy[state] = 3 # Move Right\n",
    "        else:\n",
    "            deterministic_policy[state] = 1 # Move Down (if at right edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d41470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy\n",
    "estimated_V = policy_evaluation(deterministic_policy)\n",
    "\n",
    "print(\"Estimated State-Value Function (V):\")\n",
    "print(estimated_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956d7a9",
   "metadata": {},
   "source": [
    "### function to plot the grid world policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_policy(policy, title):\n",
    "    # Visualize policy\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    \n",
    "    # Create a background grid for visualization\n",
    "    background = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "    ax.imshow(background, cmap='RdYlGn', vmin=-5, vmax=10, alpha=0.3)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(GRID_HEIGHT + 1):\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=1)\n",
    "    for j in range(GRID_WIDTH + 1):\n",
    "        ax.axvline(j - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Show policy arrows\n",
    "    action_symbols = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
    "    for row in range(GRID_HEIGHT):\n",
    "        for col in range(GRID_WIDTH):\n",
    "            state = (row, col)\n",
    "            if state != GOAL_STATE:\n",
    "                action = policy[state]\n",
    "                symbol = action_symbols[action]\n",
    "                size = 100\n",
    "                color = 'red'\n",
    "                ax.text(col, row, symbol, ha='center', va='center', \n",
    "                        fontsize=size//3, weight='bold', color=color)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, weight='bold')\n",
    "    \n",
    "    # Remove axes as requested\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_policy(deterministic_policy, \"Deterministic Policy Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce92d2",
   "metadata": {},
   "source": [
    "### function to plot the value function of the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d3472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_value_function(V, title):\n",
    "    # Visualize value function\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    \n",
    "    # Create a background grid for visualization\n",
    "    background = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "    ax.imshow(background, cmap='RdYlGn', vmin=-5, vmax=10, alpha=0.3)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(GRID_HEIGHT + 1):\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=1)\n",
    "    for j in range(GRID_WIDTH + 1):\n",
    "        ax.axvline(j - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Show value function as text\n",
    "    for row in range(GRID_HEIGHT):\n",
    "        for col in range(GRID_WIDTH):\n",
    "            state = (row, col)\n",
    "            value = V[state]\n",
    "            ax.text(col, row, f\"{value:.2f}\", ha='center', va='center', \n",
    "                    fontsize=12, weight='bold', color='blue')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, weight='bold')\n",
    "    \n",
    "    # Remove axes as requested\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b833917",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_test = policy_evaluation(deterministic_policy)\n",
    "show_value_function(v_test, \"Value Function of the Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34164778",
   "metadata": {},
   "source": [
    "### function to improve the policy based on the value function (greedy policy improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe134c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_improvement(V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Performs policy improvement based on the current value function V.\n",
    "    Args:\n",
    "        V (numpy.ndarray): The current state-value function.\n",
    "        gamma (float): Discount factor.\n",
    "    Returns:\n",
    "        dict: A new policy mapping states to action indices.\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    \n",
    "    for r in range(GRID_HEIGHT):\n",
    "        for c in range(GRID_WIDTH):\n",
    "            state = (r, c)\n",
    "            if state == GOAL_STATE:\n",
    "                new_policy[state] = 0 # Action doesn't matter at goal\n",
    "                continue\n",
    "            \n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            \n",
    "            for action_idx in range(NUM_ACTIONS):\n",
    "                next_state, reward = get_next_state_and_reward(state, action_idx)\n",
    "                value = reward + gamma * V[next_state]\n",
    "                \n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action_idx\n",
    "            \n",
    "            new_policy[state] = best_action\n",
    "    \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ad45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show improved policy\n",
    "improved_policy = greedy_policy_improvement(v_test)\n",
    "show_policy(improved_policy, \"Improved Policy Based on Value Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_policy = policy_evaluation(improved_policy)\n",
    "show_value_function(evaluated_policy, \"Value Function of the Improved Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98c7d4",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_policy = deterministic_policy\n",
    "improved_policy = starting_policy\n",
    "\n",
    "for i in range(4):  # Run for a fixed number of iterations\n",
    "    v = policy_evaluation(improved_policy)\n",
    "    # Show the improved policy\n",
    "    show_policy(improved_policy, f\"Improved Policy Iteration {i}\")\n",
    "    # Show the value function of the current policy\n",
    "    show_value_function(v, f\"Value Function Iteration {i}\")\n",
    "    # line breaks\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    improved_policy = greedy_policy_improvement(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2df2fd",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4225d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma=0.9, theta=1e-6, max_iterations=8):\n",
    "    \"\"\"\n",
    "    Performs value iteration to find the optimal value function and policy.\n",
    "    Args:\n",
    "        gamma (float): Discount factor.\n",
    "        theta (float): Threshold for convergence.\n",
    "        max_iterations (int): Maximum number of iterations to prevent infinite loops.\n",
    "    Returns:\n",
    "        tuple: (V, policy) where V is the optimal value function and policy is the optimal policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "    \n",
    "    # Set goal state value\n",
    "    V[GOAL_STATE] = REWARD_GOAL\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for r in range(GRID_HEIGHT):\n",
    "            for c in range(GRID_WIDTH):\n",
    "                state = (r, c)\n",
    "                if state == GOAL_STATE:\n",
    "                    continue  # Skip goal state as its value is already set\n",
    "                \n",
    "                v = V[r, c]\n",
    "                \n",
    "                # Find the maximum value over all possible actions\n",
    "                max_value = float('-inf')\n",
    "                for action_idx in range(NUM_ACTIONS):\n",
    "                    next_state, reward = get_next_state_and_reward(state, action_idx)\n",
    "                    value = reward + gamma * V[next_state]\n",
    "                    max_value = max(max_value, value)\n",
    "                \n",
    "                V_new[r, c] = max_value\n",
    "        \n",
    "        V = V_new\n",
    "    \n",
    "    # Extract the optimal policy from the optimal value function\n",
    "    optimal_policy = {}\n",
    "    for r in range(GRID_HEIGHT):\n",
    "        for c in range(GRID_WIDTH):\n",
    "            state = (r, c)\n",
    "            if state == GOAL_STATE:\n",
    "                optimal_policy[state] = 0  # Action doesn't matter at goal\n",
    "                continue\n",
    "            \n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            \n",
    "            for action_idx in range(NUM_ACTIONS):\n",
    "                next_state, reward = get_next_state_and_reward(state, action_idx)\n",
    "                value = reward + gamma * V[next_state]\n",
    "                \n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action_idx\n",
    "            \n",
    "            optimal_policy[state] = best_action\n",
    "    \n",
    "    return V, optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run value iteration\n",
    "print(\"Running Value Iteration...\")\n",
    "optimal_V, optimal_policy = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37edd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(optimal_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40973d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimal policy\n",
    "show_policy(optimal_policy, \"Optimal Policy (Value Iteration)\")\n",
    "\n",
    "# Visualize the optimal value function\n",
    "show_value_function(optimal_V, \"Optimal Value Function (Value Iteration)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Show convergence by running value iteration with visualization\n",
    "def value_iteration_with_visualization(gamma=0.9, theta=1e-6, max_iterations=10):\n",
    "    \"\"\"\n",
    "    Performs value iteration with step-by-step visualization.\n",
    "    \"\"\"\n",
    "    V = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "    V[GOAL_STATE] = REWARD_GOAL\n",
    "    \n",
    "    # Show initial state\n",
    "    show_value_function(V, \"Value Function - Initial State\")\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for r in range(GRID_HEIGHT):\n",
    "            for c in range(GRID_WIDTH):\n",
    "                state = (r, c)\n",
    "                if state == GOAL_STATE:\n",
    "                    continue\n",
    "                \n",
    "                v = V[r, c]\n",
    "                \n",
    "                # Find the maximum value over all possible actions\n",
    "                max_value = float('-inf')\n",
    "                for action_idx in range(NUM_ACTIONS):\n",
    "                    next_state, reward = get_next_state_and_reward(state, action_idx)\n",
    "                    value = reward + gamma * V[next_state]\n",
    "                    max_value = max(max_value, value)\n",
    "                \n",
    "                V_new[r, c] = max_value\n",
    "        \n",
    "        V = V_new\n",
    "        \n",
    "        # Extract current policy\n",
    "        current_policy = {}\n",
    "        for r in range(GRID_HEIGHT):\n",
    "            for c in range(GRID_WIDTH):\n",
    "                state = (r, c)\n",
    "                if state == GOAL_STATE:\n",
    "                    current_policy[state] = 0\n",
    "                    continue\n",
    "                \n",
    "                best_action = None\n",
    "                best_value = float('-inf')\n",
    "                \n",
    "                for action_idx in range(NUM_ACTIONS):\n",
    "                    next_state, reward = get_next_state_and_reward(state, action_idx)\n",
    "                    value = reward + gamma * V[next_state]\n",
    "                    \n",
    "                    if value > best_value:\n",
    "                        best_value = value\n",
    "                        best_action = action_idx\n",
    "                \n",
    "                current_policy[state] = best_action\n",
    "        \n",
    "        # Show current state\n",
    "        show_policy(current_policy, f\"Policy - Iteration {iteration + 1}\")\n",
    "        show_value_function(V, f\"Value Function - Iteration {iteration + 1}\")\n",
    "        \n",
    "        print(f\"Iteration {iteration + 1}\")\n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    return V, current_policy\n",
    "\n",
    "\n",
    "# Run value iteration with visualization\n",
    "print(\"\\n\\nRunning Value Iteration with Visualization...\")\n",
    "final_V, final_policy = value_iteration_with_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b61bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
