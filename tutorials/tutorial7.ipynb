{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67dfc6",
   "metadata": {},
   "source": [
    "# Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Cliff Walking environment parameters\n",
    "GRID_HEIGHT = 4\n",
    "GRID_WIDTH = 4\n",
    "START_STATE = (0, 0)\n",
    "GOAL_STATE = (3, 3)\n",
    "CLIFF_ONE = (1, 1)\n",
    "CLIFF_TWO = (2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions: 0: Up, 1: Down, 2: Left, 3: Right # nothing\n",
    "ACTIONS = [( -1, 0), (1, 0), (0, -1), (0, 1), (0, 0)] # delta_row, delta_col\n",
    "NUM_ACTIONS = len(ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "REWARD_NORMAL = -1\n",
    "REWARD_GOAL = 10\n",
    "REWARD_CLIFF = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state):\n",
    "    row, col = state\n",
    "    return 0 <= row < GRID_HEIGHT and 0 <= col < GRID_WIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb9b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(state):\n",
    "    valid_actions = []\n",
    "    if state == GOAL_STATE or state == CLIFF_ONE or state == CLIFF_TWO:\n",
    "        return [4]  # Only 'do nothing' action is valid\n",
    "    for action in range(NUM_ACTIONS-1):  # Exclude the 'do nothing' action\n",
    "        new_state = (state[0] + ACTIONS[action][0], state[1] + ACTIONS[action][1])\n",
    "        if is_valid_state(new_state):\n",
    "            valid_actions.append(action)\n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state_and_reward(state, action_idx):\n",
    "    row, col = state\n",
    "    delta_row, delta_col = ACTIONS[action_idx]\n",
    "    next_row, next_col = row + delta_row, col + delta_col\n",
    "\n",
    "    if not is_valid_state((next_row, next_col)):\n",
    "        # Stay in the same state if moving out of bounds\n",
    "        next_state = state\n",
    "        reward = REWARD_NORMAL\n",
    "    elif (next_row, next_col) == CLIFF_ONE or (next_row, next_col) == CLIFF_TWO:\n",
    "        # Fell off the cliff\n",
    "        next_state = START_STATE\n",
    "        reward = REWARD_CLIFF + REWARD_NORMAL  # -1 for the step, plus cliff penalty\n",
    "    elif (next_row, next_col) == GOAL_STATE:\n",
    "        # Reached the goal\n",
    "        next_state = GOAL_STATE # Stays at goal after reaching\n",
    "        reward = REWARD_NORMAL # Still -1 per step, but episode ends\n",
    "    else:\n",
    "        # Normal movement\n",
    "        next_state = (next_row, next_col)\n",
    "        reward = REWARD_NORMAL\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a991727",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_policy(policy, title):\n",
    "    # Visualize policy\n",
    "    _, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    \n",
    "    # Create a background grid for visualization\n",
    "    background = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "    ax.imshow(background, cmap='RdYlGn', vmin=-5, vmax=10, alpha=0.3)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(GRID_HEIGHT + 1):\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=1)\n",
    "    for j in range(GRID_WIDTH + 1):\n",
    "        ax.axvline(j - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Show policy arrows\n",
    "    action_symbols = {0: '↑', 1: '↓', 2: '←', 3: '→', 4: ' '}\n",
    "    for row in range(GRID_HEIGHT):\n",
    "        for col in range(GRID_WIDTH):\n",
    "            state = (row, col)\n",
    "            if state != GOAL_STATE:\n",
    "                action = policy(state)\n",
    "                symbol = action_symbols[action]\n",
    "                size = 100\n",
    "                color = 'red'\n",
    "                ax.text(col, row, symbol, ha='center', va='center', \n",
    "                        fontsize=size//3, weight='bold', color=color)\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, weight='bold')\n",
    "    \n",
    "    # Remove axes as requested\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e769510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_value_function(V, title):\n",
    "    # Visualize value function\n",
    "    _, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    \n",
    "    # Create a background grid for visualization\n",
    "    background = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "    ax.imshow(background, cmap='RdYlGn', vmin=-5, vmax=10, alpha=0.3)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(GRID_HEIGHT + 1):\n",
    "        ax.axhline(i - 0.5, color='black', linewidth=1)\n",
    "    for j in range(GRID_WIDTH + 1):\n",
    "        ax.axvline(j - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Show value function as text\n",
    "    for row in range(GRID_HEIGHT):\n",
    "        for col in range(GRID_WIDTH):\n",
    "            state = (row, col)\n",
    "            value = V[state]\n",
    "            ax.text(col, row, f\"{value:.2f}\", ha='center', va='center', \n",
    "                    fontsize=12, weight='bold', color='blue')\n",
    "    \n",
    "    ax.set_title(title, fontsize=14, weight='bold')\n",
    "    \n",
    "    # Remove axes as requested\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ba456",
   "metadata": {},
   "source": [
    "# Epsilon-greedy policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0672949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, q_values, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explore: select a random action\n",
    "        valid_actions = get_valid_actions(state)\n",
    "        return np.random.choice(valid_actions) if valid_actions else 4\n",
    "    else:\n",
    "        # Exploit: select the best action\n",
    "        return np.argmax(q_values[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807885ed",
   "metadata": {},
   "source": [
    "## Extract Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541853a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(q_values):\n",
    "    policy_dict = {}\n",
    "    for row in range(GRID_HEIGHT):\n",
    "        for col in range(GRID_WIDTH):\n",
    "            state = (row, col)\n",
    "            if state != GOAL_STATE and state != CLIFF_ONE and state != CLIFF_TWO:\n",
    "                ordered_actions = np.argsort(q_values[state][:-1])[::-1]\n",
    "                best_action = None\n",
    "                for action in ordered_actions:\n",
    "                    if action in get_valid_actions(state):\n",
    "                        best_action = action\n",
    "                        break\n",
    "                if best_action is None:\n",
    "                    # If no valid action found, default to 'do nothing'\n",
    "                    best_action = 4\n",
    "                policy_dict[state] = best_action\n",
    "            else:\n",
    "                policy_dict[state] = 4\n",
    "    def policy(state):\n",
    "        return policy_dict.get(state, None)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fcba6",
   "metadata": {},
   "source": [
    "## Extract Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8bdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(q_values):\n",
    "    V = np.zeros((GRID_HEIGHT, GRID_WIDTH))\n",
    "    for row in range(GRID_HEIGHT):\n",
    "        for col in range(GRID_WIDTH):\n",
    "            state = (row, col)\n",
    "            if state != GOAL_STATE and state != CLIFF_ONE and state != CLIFF_TWO:\n",
    "                q_value = q_values[state][:-1]\n",
    "                actions = np.argsort(q_value)[::-1]\n",
    "                value = None\n",
    "                for action in actions:  # Exclude the 'do nothing' action\n",
    "                    if action in get_valid_actions(state):\n",
    "                        value = q_value[action]\n",
    "                if value is not None:\n",
    "                    V[state] = value\n",
    "                else:\n",
    "                    V[state] = 0\n",
    "            elif state == GOAL_STATE:\n",
    "                V[state] = 10\n",
    "            else:\n",
    "                V[state] = -5\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cbe380",
   "metadata": {},
   "source": [
    "## Q-Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b817fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(num_episodes, alpha, gamma, epsilon, echo_interval):\n",
    "    # Initialize Q-values\n",
    "    q_values = np.zeros((GRID_HEIGHT, GRID_WIDTH, NUM_ACTIONS))\n",
    "    for episode in range(num_episodes):\n",
    "        state = START_STATE\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = epsilon_greedy_policy(state, q_values, epsilon)\n",
    "            next_state, reward = get_next_state_and_reward(state, action)\n",
    "            \n",
    "            # Update Q-value\n",
    "            best_next_action = np.argmax(q_values[next_state])\n",
    "            td_target = reward + gamma * q_values[next_state][best_next_action]\n",
    "            td_error = td_target - q_values[state][action]\n",
    "            q_values[state][action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if state == GOAL_STATE:\n",
    "                done = True\n",
    "        if episode % echo_interval == 0:\n",
    "            print(f\"Episode {episode}: Q-values updated.\")\n",
    "            # Extract the policy from Q-values\n",
    "            show_policy(extract_policy(q_values), f\"Q-learning Policy at Episode {episode}\")\n",
    "            show_value_function(value_function(q_values), f\"Q-learning Value Function at Episode {episode}\")\n",
    "    # Final policy and value function\n",
    "    final_policy = extract_policy(q_values)\n",
    "    final_value_function = value_function(q_values)\n",
    "    show_policy(final_policy, \"Q-learning Final Policy\")\n",
    "    show_value_function(final_value_function, \"Q-learning Final Value Function\")\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b677326",
   "metadata": {},
   "source": [
    "## SARSA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(num_episodes, alpha, gamma, epsilon, echo_interval):\n",
    "    q_values = np.zeros((GRID_HEIGHT, GRID_WIDTH, NUM_ACTIONS))\n",
    "    for episode in range(num_episodes):\n",
    "        state = START_STATE\n",
    "        action = epsilon_greedy_policy(state, q_values, epsilon)\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward = get_next_state_and_reward(state, action)\n",
    "            next_action = epsilon_greedy_policy(next_state, q_values, epsilon)\n",
    "            \n",
    "            # Update Q-value\n",
    "            td_target = reward + gamma * q_values[next_state][next_action]\n",
    "            td_error = td_target - q_values[state][action]\n",
    "            q_values[state][action] += alpha * td_error\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "            if state == GOAL_STATE:\n",
    "                done = True\n",
    "        if episode % echo_interval == 0:\n",
    "            print(f\"Episode {episode}/{num_episodes} completed.\")\n",
    "            show_policy(extract_policy(q_values), f\"SARSA Policy at Episode {episode}\")\n",
    "            show_value_function(value_function(q_values), f\"SARSA Value Function at Episode {episode}\")\n",
    "\n",
    "    # Final policy and value function\n",
    "    final_policy = extract_policy(q_values)\n",
    "    final_value_function = value_function(q_values)\n",
    "    show_policy(final_policy, \"SARSA Final Policy\")\n",
    "    show_value_function(final_value_function, \"SARSA Final Value Function\")\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac206e",
   "metadata": {},
   "source": [
    "## launching the training Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e20e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning(num_episodes=500, alpha=0.1, gamma=0.9, epsilon=0.1, echo_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5c972",
   "metadata": {},
   "source": [
    "## SARSA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa(num_episodes=1000, alpha=0.1, gamma=0.9, epsilon=0.1, echo_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5e5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
